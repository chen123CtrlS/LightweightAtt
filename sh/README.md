## Experiments

Table1:
roberta-base-lora.sh
llama3.1-8b-lora.sh

Figure1:
exp2_pre_roberta.sh
exp2_roberta.sh

Figure2:
exp2_pre_llama3.sh
exp2_llama3.sh

Table2:
exp3_1.sh
exp3_2.sh

## Citation

Our code references third-party codes released in the following articles.
```bash
@misc{hayou2024loraefficientlowrank,
      title={LoRA+: Efficient Low Rank Adaptation of Large Models}, 
      author={Soufiane Hayou and Nikhil Ghosh and Bin Yu},
      year={2024},
      eprint={2402.12354},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12354}, 
}
@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}
```